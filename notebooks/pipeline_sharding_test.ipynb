{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "# Logical cores (includes hyperthreads)\n",
    "print(\"Logical cores:\", os.cpu_count())\n",
    "\n",
    "\n",
    "# Total threads/cores via multiprocessing\n",
    "print(\"multiprocessing.cpu_count():\", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "# use dotenv to handle env variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "env_loaded =load_dotenv(dotenv_path='./data.env')\n",
    "assert env_loaded, \"Failed to load .env file\"\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding\n",
    "\n",
    "from rubix.core.pipeline import RubixPipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "print(jax.devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# RUBIX pipeline\n",
    "\n",
    "RUBIX is designed as a linear pipeline, where the individual functions are called and constructed as a pipeline. This allows as to execute the whole data transformation from a cosmological hydrodynamical simulation of a galaxy to an IFU cube in two lines of code. This notebook shows, how to execute the pipeline on multiple machines. To see, how the pipeline is executed in small individual steps per individual function, we refer to the notebook `rubix_pipeline_stepwise.ipynb`.\n",
    "\n",
    "## How to use the Pipeline\n",
    "1) Define a `config`\n",
    "2) Setup the `pipeline yaml`\n",
    "3) Run the RUBIX pipeline\n",
    "4) Do science with the mock-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 1: Config\n",
    "\n",
    "The `config` contains all the information needed to run the pipeline. Those are run specfic configurations. Currently we just support Illustris as simulation, but extensions to other simulations (e.g. NIHAO) are planned.\n",
    "\n",
    "For the `config` you can choose the following options:\n",
    "- `pipeline`: you specify the name of the pipeline that is stored in the yaml file in rubix/config/pipeline_config.yml\n",
    "- `logger`: RUBIX has implemented a logger to report to the user, what is happening during the pipeline execution and give warnings\n",
    "- `data - args - particle_type`: load only stars particle (\"particle_type\": [\"stars\"]) or only gas particle (\"particle_type\": [\"gas\"]) or both (\"particle_type\": [\"stars\",\"gas\"])\n",
    "- `data - args - simulation`: choose the Illustris simulation (e.g. \"simulation\": \"TNG50-1\")\n",
    "- `data - args - snapshot`: which time step of the simulation (99 for present day)\n",
    "- `data - args - save_data_path`: set the path to save the downloaded Illustris data\n",
    "- `data - load_galaxy_args - id`: define, which Illustris galaxy is downloaded\n",
    "- `data - load_galaxy_args - reuse`: if True, if in the save_data_path directory a file for this galaxy id already exists, the downloading is skipped and the preexisting file is used\n",
    "- `data - subset`: only a defined number of stars/gas particles is used and stored for the pipeline. This may be helpful for quick testing\n",
    "- `simulation - name`: currently only IllustrisTNG is supported\n",
    "- `simulation - args - path`: where the data is stored and how the file will be named\n",
    "- `output_path`: where the hdf5 file is stored, which is then the input to the RUBIX pipeline\n",
    "- `telescope - name`: define the telescope instrument that is observing the simulation. Some telescopes are predefined, e.g. MUSE. If your instrument does not exist predefined, you can easily define your instrument in rubix/telescope/telescopes.yaml\n",
    "- `telescope - psf`: define the point spread function that is applied to the mock data\n",
    "- `telescope - lsf`: define the line spread function that is applied to the mock data\n",
    "- `telescope - noise`: define the noise that is applied to the mock data\n",
    "- `cosmology`: specify the cosmology you want to use, standard for RUBIX is \"PLANCK15\"\n",
    "- `galaxy - dist_z`: specify at which redshift the mock-galaxy is observed\n",
    "- `galaxy - rotation`: specify the orientation of the galaxy. You can set the types edge-on or face-on or specify the angles alpha, beta and gamma as rotations around x-, y- and z-axis\n",
    "- `ssp - template`: specify the simple stellar population lookup template to get the stellar spectrum for each stars particle. In RUBIX frequently \"BruzualCharlot2003\" is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"pipeline\":{\"name\": \"calc_ifu\"},\n",
    "    \n",
    "    \"logger\": {\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"log_file_path\": None,\n",
    "        \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"name\": \"IllustrisAPI\",\n",
    "        \"args\": {\n",
    "            \"api_key\": os.environ.get(\"ILLUSTRIS_API_KEY\"),\n",
    "            \"particle_type\": [\"stars\"],\n",
    "            \"simulation\": \"TNG50-1\",\n",
    "            \"snapshot\": 99,\n",
    "            \"save_data_path\": \"data\",\n",
    "        },\n",
    "        \n",
    "        \"load_galaxy_args\": {\n",
    "        \"id\": 14,\n",
    "        \"reuse\": True,\n",
    "        },\n",
    "        \n",
    "        \"subset\": {\n",
    "            \"use_subset\": True,\n",
    "            \"subset_size\": 30000,\n",
    "        },\n",
    "    },\n",
    "    \"simulation\": {\n",
    "        \"name\": \"IllustrisTNG\",\n",
    "        \"args\": {\n",
    "            \"path\": \"data/galaxy-id-14.hdf5\",\n",
    "        },\n",
    "    \n",
    "    },\n",
    "    \"output_path\": \"output\",\n",
    "\n",
    "    \"telescope\":\n",
    "        {\"name\": \"MUSE\",\n",
    "         \"psf\": {\"name\": \"gaussian\", \"size\": 5, \"sigma\": 0.6},\n",
    "         \"lsf\": {\"sigma\": 0.5},\n",
    "         \"noise\": {\"signal_to_noise\": 100,\"noise_distribution\": \"normal\"},},\n",
    "    \"cosmology\":\n",
    "        {\"name\": \"PLANCK15\"},\n",
    "        \n",
    "    \"galaxy\":\n",
    "        {\"dist_z\": 0.1,\n",
    "         \"rotation\": {\"type\": \"edge-on\"},\n",
    "        },\n",
    "    \"ssp\": {\n",
    "        \"template\": {\n",
    "            \"name\": \"FSPS\"\n",
    "        },\n",
    "        \"dust\": {\n",
    "                \"extinction_model\": \"Cardelli89\",\n",
    "                \"dust_to_gas_ratio\": 0.01,\n",
    "                \"dust_to_metals_ratio\": 0.4,\n",
    "                \"dust_grain_density\": 3.5,\n",
    "                \"Rv\": 3.1,\n",
    "            },\n",
    "    },        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 2: Pipeline yaml\n",
    "\n",
    "To run the RUBIX pipeline, you need a yaml file (stored in `rubix/config/pipeline_config.yml`) that defines which functions are used during the execution of the pipeline. This shows the example pipeline yaml to compute a stellar IFU cube.\n",
    "\n",
    "```yaml\n",
    "calc_ifu:\n",
    "  Transformers:\n",
    "    rotate_galaxy:\n",
    "      name: rotate_galaxy\n",
    "      depends_on: null\n",
    "      args: []\n",
    "      kwargs:\n",
    "        type: \"face-on\"\n",
    "    filter_particles:\n",
    "      name: filter_particles\n",
    "      depends_on: rotate_galaxy\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    spaxel_assignment:\n",
    "      name: spaxel_assignment\n",
    "      depends_on: filter_particles\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    reshape_data:\n",
    "      name: reshape_data\n",
    "      depends_on: spaxel_assignment\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    calculate_spectra:\n",
    "      name: calculate_spectra\n",
    "      depends_on: reshape_data\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    scale_spectrum_by_mass:\n",
    "      name: scale_spectrum_by_mass\n",
    "      depends_on: calculate_spectra\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    doppler_shift_and_resampling:\n",
    "      name: doppler_shift_and_resampling\n",
    "      depends_on: scale_spectrum_by_mass\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    calculate_datacube:\n",
    "      name: calculate_datacube\n",
    "      depends_on: doppler_shift_and_resampling\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    convolve_psf:\n",
    "      name: convolve_psf\n",
    "      depends_on: calculate_datacube\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    convolve_lsf:\n",
    "      name: convolve_lsf\n",
    "      depends_on: convolve_psf\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    apply_noise:\n",
    "      name: apply_noise\n",
    "      depends_on: convolve_lsf\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "```\n",
    "\n",
    "There is one thing you have to know about the naming of the functions in this yaml: To use the functions inside the pipeline, the functions have to be called exactly the same as they are returned from the core module function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Data organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "try simple approach for this thing for now. This is really stupid: just build a giant box of zeros, index into them in the right way, and use these indices to assign the values we want to slices in the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "\n",
    "# this function builds the data from the rubixdata object because that is easiest, but should not really be done imho. \n",
    "def build_data(inputdata): \n",
    "    long_axis = inputdata.stars.age.shape[0]\n",
    "    data = jnp.zeros((long_axis, 6200), dtype=jnp.float32)\n",
    "    inputdata.galaxy.redshift = jnp.float32(inputdata.galaxy.redshift)\n",
    "    inputdata.galaxy.halfmassrad_stars = jnp.array(inputdata.galaxy.halfmassrad_stars, dtype=jnp.float32)\n",
    "    inputdata.galaxy.center = jnp.array(inputdata.galaxy.center, dtype=jnp.float32)\n",
    "\n",
    "    inputdata.stars.coords = jnp.array(inputdata.stars.coords, dtype=jnp.float32)\n",
    "    inputdata.stars.age = jnp.array(inputdata.stars.age, dtype=jnp.float32)\n",
    "    inputdata.stars.velocity = jnp.array(inputdata.stars.velocity, dtype=jnp.float32)\n",
    "    inputdata.stars.metallicity = jnp.array(inputdata.stars.metallicity, dtype=jnp.float32)\n",
    "    inputdata.stars.mass = jnp.array(inputdata.stars.mass, dtype=jnp.float32)\n",
    "    # stars properties\n",
    "    data = data.at[:, 0:3].set(inputdata.stars.coords)\n",
    "    data = data.at[:, 3:6].set(inputdata.stars.velocity)\n",
    "    data = data.at[:, 6].set(inputdata.stars.metallicity)\n",
    "    data = data.at[:, 7].set(inputdata.stars.age)\n",
    "    data = data.at[:, 8].set(inputdata.stars.mass)\n",
    "\n",
    "    # galaxy properties\n",
    "    data = data.at[:, 9].set(inputdata.galaxy.halfmassrad_stars)\n",
    "    data = data.at[:, 10].set(inputdata.galaxy.redshift)\n",
    "    data = data.at[:, 11:14].set(inputdata.galaxy.center)\n",
    "    \n",
    "    mesh = jax.make_mesh((jax.device_count(), ), ('x',))\n",
    "    shard = NamedSharding(mesh, P('x'))\n",
    "\n",
    "    data = jax.device_put(data, shard)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "def stars(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Stars function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    # Perform some operations on the data\n",
    "    # For example, let's just return the data as is\n",
    "    return data[:, 0:9]\n",
    "\n",
    "def gas(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    return data # index after adjusting the above for gas\n",
    "\n",
    "def galaxy(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Galaxy function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    # Perform some operations on the data\n",
    "    # For example, let's just return the data as is\n",
    "    return data[:, 9:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "def coords_idx(): \n",
    "    return jnp.s_[:, 0:3]\n",
    "\n",
    "def coords(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Coords function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[coords_idx()]\n",
    "\n",
    "def velocity_idx():\n",
    "    return jnp.s_[:, 3:6]\n",
    "\n",
    "def velocity(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Velocity function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[velocity_idx()]\n",
    "\n",
    "def metallicity_idx():\n",
    "    return jnp.s_[:, 6]\n",
    "\n",
    "def metallicity(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Metallicity function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[metallicity_idx()]\n",
    "\n",
    "def age_idx():\n",
    "    return jnp.s_[:, 7]\n",
    "\n",
    "def age(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Age function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[age_idx()]\n",
    "\n",
    "def mass_idx():\n",
    "    return jnp.s_[:, 8]\n",
    "\n",
    "def mass(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Age function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[mass_idx()]\n",
    "\n",
    "def halfmassrad_stars_idx():\n",
    "    return jnp.s_[:, 9]\n",
    "\n",
    "def halfmassrad_stars(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Halfmassrad_stars function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[halfmassrad_stars_idx()]\n",
    "\n",
    "\n",
    "def redshift_idx():\n",
    "    return jnp.s_[:, 10]\n",
    "\n",
    "def redshift(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Redshift function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[redshift_idx()]\n",
    "\n",
    "def center_idx():\n",
    "    return jnp.s_[:, 11:14]\n",
    "\n",
    "def center(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Center function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[center_idx()]\n",
    "\n",
    "def mask_idx() :\n",
    "    return jnp.s_[:, 14]\n",
    "\n",
    "def mask(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Mask function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[mask_idx()]\n",
    "\n",
    "def pixel_assignment_idx() : \n",
    "    return jnp.s_[:, 15]\n",
    "\n",
    "def pixel_assignment(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Pixel assignment function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[pixel_assignment_idx()]\n",
    "\n",
    "\n",
    "def spectra_index(): \n",
    "    return jnp.s_[:, 16:(16 + 5994)]\n",
    "\n",
    "def spectra(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Spectra function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[spectra_index()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "try the sharding now with pipeline functions. since the pipeline functions use other data, I don´t use them directly, but build simplified versions here that only include stars. this involves the build up of the pipeline from the ground up in such a way that the data is sharded once and then we don´t have to touch it again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "TODO: make sure the functions have the correct static argnums such that we don´t have to worry about the tracing shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "from functools import partial\n",
    "from pipe import Pipe\n",
    "from rubix.galaxy.alignment import moment_of_inertia_tensor, rotation_matrix_from_inertia_tensor, apply_init_rotation, apply_rotation\n",
    "from rubix.core.telescope import get_spatial_bin_edges\n",
    "from rubix.telescope.utils import mask_particles_outside_aperture\n",
    "from rubix.core.pipeline import RubixPipeline \n",
    "from rubix.core.data import RubixData\n",
    "from rubix.core.telescope import get_telescope\n",
    "from jax import random as jrandom\n",
    "from rubix.core.ssp import get_ssp, get_lookup_interpolation\n",
    "from rubix.telescope.psf.kernels import gaussian_kernel_2d\n",
    "from jax.scipy.signal import convolve2d\n",
    "from rubix.telescope.lsf.lsf import _get_kernel\n",
    "from jax.scipy.signal import convolve\n",
    "from rubix import config as rubix_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## galaxy rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "def rotate_galaxy_impl(data: jnp.array, alpha, beta, gamma)->jnp.array: \n",
    "\n",
    "    I = moment_of_inertia_tensor(coords(data), mass(data), halfmassrad_stars(data),)\n",
    "    R = rotation_matrix_from_inertia_tensor(I)\n",
    "    data = data.at[coords_idx()].set(apply_rotation(apply_init_rotation(coords(data), R), alpha, beta, gamma))\n",
    "    data = data.at[velocity_idx()].set(apply_rotation(apply_init_rotation(velocity(data), R), alpha, beta, gamma))\n",
    "    return data\n",
    "\n",
    "# TODO: generalize, get these numbers from the config\n",
    "rotate_galaxy = partial(rotate_galaxy_impl, alpha=90.0, beta=0.0, gamma=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## filter particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NBVAL_SKIP\n",
    "\n",
    "def filter_particles_impl(data: jnp.ndarray, spatial_bin_edges) -> jnp.ndarray:\n",
    "    mask = mask_particles_outside_aperture(\n",
    "        coords(data), spatial_bin_edges\n",
    "    )\n",
    "\n",
    "    data = data.at[mask_idx()].set(mask)\n",
    "\n",
    "    for attr in [age_idx, mass_idx, metallicity_idx, ]: \n",
    "        data = data.at[attr()].set(\n",
    "            jnp.where(mask, data[attr()], 0)\n",
    "        )\n",
    "\n",
    "    return data\n",
    "\n",
    "filter_particles = partial(filter_particles_impl, spatial_bin_edges=get_spatial_bin_edges(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## spaxel assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "def spaxel_assignment_square_impl(data: jnp.ndarray, spatial_bin_edges)-> jnp.ndarray:\n",
    "    # Calculate assignment of of x and y coordinates to bins separately\n",
    "    x_indices = (\n",
    "        jnp.digitize(data[coords_idx()][:, 0], spatial_bin_edges) - 1\n",
    "    )  # -1 to start indexing at 0\n",
    "    y_indices = jnp.digitize(data[coords_idx()][:, 1], spatial_bin_edges) - 1\n",
    "\n",
    "    number_of_bins = len(spatial_bin_edges) - 1\n",
    "\n",
    "    # Clip the indices to the valid range\n",
    "    x_indices = jnp.clip(x_indices, 0, number_of_bins - 1)\n",
    "    y_indices = jnp.clip(y_indices, 0, number_of_bins - 1)\n",
    "\n",
    "    # Flatten the 2D indices to 1D indices\n",
    "    pixel_positions = x_indices + (number_of_bins * y_indices)\n",
    "    return data.at[pixel_assignment_idx()].set(jnp.round(pixel_positions))\n",
    "\n",
    "\n",
    "spaxel_assignment = partial(spaxel_assignment_square_impl, spatial_bin_edges=get_spatial_bin_edges(config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Calculate spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "calculate spectra now. since this is so big, it would perpaps make sense to have a separate path for this thing instead of having to save this and drag it around all the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "# this needs to be optimized, it uses far too much memory\n",
    "def calculate_spectra_impl(data: jnp.ndarray, lookup_interpolation) -> jnp.ndarray: \n",
    "    print(\"Calculating spectra\")\n",
    "    print(\"Data shape:\", data.shape)\n",
    "    print(\"lookup type: \", type(lookup_interpolation))\n",
    "    print(\"lookup shape: \", lookup_interpolation.shape)\n",
    "    # this thing is gigantic and probably cannot be stored in memory for serious data\n",
    "    return data.at[spectra_index()].set(lookup_interpolation(\n",
    "        data[metallicity_idx()],\n",
    "        data[age_idx()],\n",
    "    ))\n",
    "# this creates a file access that should not be on the hot path. \n",
    "lookup_interpolation = get_lookup_interpolation(config)\n",
    "calculate_spectra = partial(calculate_spectra_impl, lookup_interpolation=lookup_interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## scale spectrum by mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "def scale_spectrum_by_mass(data: jnp.ndarray) -> jnp.ndarray:\n",
    "\n",
    "    return data.at[spectra_index()].set(\n",
    "        data[spectra_index()] * data[mass_idx()][:, jnp.newaxis]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## doppler shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "# get all the needed crap... \n",
    "velocity_direction = rubix_config[\"ifu\"][\"doppler\"][\"velocity_direction\"]\n",
    "directions = {\"x\": 0, \"y\": 1, \"z\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "# TODO: this needs to be fused with the resampling step such that the giant temporary array is not created\n",
    "def apply_doppler_impl(data: jnp.ndarray, wavelength, c, direction) -> jnp.ndarray:\n",
    "\n",
    "    # 3 is the index of the first velocity component\n",
    "    d = jnp.exp(data[:, 3 + direction]/ c) # 3 is offset of the velocity component\n",
    "\n",
    "    return jax.vmap(lambda d: wavelength * d)(d)\n",
    "\n",
    "ssp = get_ssp(config)\n",
    "ssp_wave= ssp.wavelength\n",
    "direction = directions[velocity_direction]\n",
    "cosmological_doppler_shift = (1 + config[\"galaxy\"][\"dist_z\"]) * ssp.wavelength\n",
    "\n",
    "apply_doppler = partial(apply_doppler_impl, wavelength=ssp_wave, c=3e8, direction=direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "def calculate_diff(\n",
    "    vec, pad_with_zero: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the difference between each element in a vector.\n",
    "\n",
    "    Args:\n",
    "        vec (array-like): The input vector.\n",
    "        pad_with_zero (bool, optional): Whether to prepend the first element of the vector to the differences. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        The differences between each element in the vector (array-like).\n",
    "    \"\"\"\n",
    "\n",
    "    if pad_with_zero:\n",
    "        differences = jnp.diff(vec, prepend=vec[0])\n",
    "    else:\n",
    "        differences = jnp.diff(vec)\n",
    "    return differences\n",
    "\n",
    "\n",
    "def resample_spectrum_impl(init_spectrum: jnp.ndarray, initial_wavelength, target_wavelength) -> jnp.ndarray:\n",
    "    in_range_mask = (initial_wavelength >= jnp.min(target_wavelength)) & (\n",
    "        initial_wavelength <= jnp.max(target_wavelength)\n",
    "    )\n",
    "\n",
    "    intrinsic_wave_diff = calculate_diff(initial_wavelength) * in_range_mask\n",
    "\n",
    "    # Get total luminsoity within the wavelength range\n",
    "    total_lum = jnp.sum(init_spectrum * intrinsic_wave_diff)\n",
    "\n",
    "    # Interpolate the wavelegnth to the telescope grid\n",
    "    particle_lum = jnp.interp(target_wavelength, initial_wavelength, init_spectrum)\n",
    "\n",
    "    # New total luminosity\n",
    "    new_total_lum = jnp.sum(particle_lum * calculate_diff(target_wavelength))\n",
    "\n",
    "    # Factor to conserve flux in the new spectrum\n",
    "    scale_factor = total_lum / new_total_lum\n",
    "    scale_factor = jnp.nan_to_num(\n",
    "        scale_factor, nan=0.0\n",
    "    )  # Otherwise we get NaNs if new_total_lum is zero\n",
    "    lum = particle_lum * scale_factor\n",
    "\n",
    "    return lum\n",
    "\n",
    "# indexing stuff for spectra\n",
    "def rs_spectra_index(out_size: int): \n",
    "    return jnp.s_[:, 16:(16 + out_size)]\n",
    "\n",
    "def diff_spectra_index(in_size: int, out_size: int): \n",
    "    return jnp.s_[:, 16:(16 + (in_size - out_size))]\n",
    "\n",
    "def rs_spectra(data: jnp.ndarray, out_size: int) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Spectra function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[rs_spectra_index(out_size)]\n",
    "\n",
    "def doppler_and_resample(data: jnp.array, target_wavelength: jnp.array,  out_size: int) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Doppler shift and resample the spectrum.\n",
    "    \"\"\"\n",
    "    # Apply the doppler shift\n",
    "    v = apply_doppler(data)\n",
    "\n",
    "    # Resample the spectrum\n",
    "    data = data.at[rs_spectra_index(out_size)].set(\n",
    "        jax.vmap(resample_spectrum_impl, in_axes=(0,0, None))(\n",
    "            data[spectra_index()], v, target_wavelength\n",
    "        )\n",
    "    )\n",
    "    data = data.at[diff_spectra_index(ssp_wave.shape[0], out_size)].set(0.0)\n",
    "\n",
    "    return data\n",
    "\n",
    "telescope = get_telescope(config)\n",
    "telescope_wavelength = telescope.wave_seq\n",
    "num_spaxels = int(telescope.sbin)\n",
    "out_size = int(telescope_wavelength.shape[0])\n",
    "\n",
    "resample = partial(doppler_and_resample,target_wavelength=telescope_wavelength, out_size = telescope_wavelength.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "get all the telescope data stuff and make a partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "telescope = get_telescope(config)\n",
    "telescope_wavelength = telescope.wave_seq\n",
    "num_spaxels = int(telescope.sbin)\n",
    "out_size = int(telescope_wavelength.shape[0])\n",
    "\n",
    "resample = partial(doppler_and_resample,target_wavelength=telescope_wavelength, out_size = telescope_wavelength.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## apply extinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "from rubix.telescope.utils import calculate_spatial_bin_edges\n",
    "from rubix.core.cosmology import get_cosmology\n",
    "from rubix.spectra.dust.extinction_models import Rv_model_dict, Cardelli89, Gordon23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "galaxy_dist_z = config[\"galaxy\"][\"dist_z\"]\n",
    "telescope = get_telescope(config)\n",
    "telescope_wavelength = telescope.wave_seq\n",
    "num_spaxels = int(telescope.sbin)\n",
    "cosmology = get_cosmology(config)\n",
    "ext_model = config[\"ssp\"][\"dust\"][\"extinction_model\"]\n",
    "Rv = config[\"ssp\"][\"dust\"][\"Rv\"]\n",
    "ext_model_class = Rv_model_dict[ext_model]\n",
    "ext = ext_model_class(Rv=Rv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "_, spatial_bin_size = calculate_spatial_bin_edges(fov =telescope.fov, spatial_bins = telescope.sbin, dist_z = galaxy_dist_z, cosmology = cosmology)\n",
    "spaxel_area = spatial_bin_size**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "def apply_extinction(data: jnp.ndarray, wavelength, spaxel_area, n_spaxels, ext) -> jnp.ndarray:\n",
    "    # I don´t have gas in the data currently, so I skip this for now. \n",
    "    # The way it is done in the dust_extinction module has config lookups within the function, and the sorting should be avoided when possible! It's not clear why this is needed? \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## calculate datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "def calculate_datacube_impl(data: jnp.ndarray, num_spaxels: int, out_size: int) -> jnp.ndarray:\n",
    "    return jax.ops.segment_sum(\n",
    "        data[rs_spectra_index(out_size)], # spectra\n",
    "        data[pixel_assignment_idx()].astype('int32'), # pixel assignment\n",
    "        num_segments=num_spaxels**2,\n",
    "    ).reshape(\n",
    "        (num_spaxels, num_spaxels, telescope_wavelength.shape[0])\n",
    "    )\n",
    "\n",
    "calculate_datacube = partial(calculate_datacube_impl, num_spaxels= int(telescope.sbin), out_size=out_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## convolve psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "m, n = config[\"telescope\"][\"psf\"][\"size\"], config[\"telescope\"][\"psf\"][\"size\"]\n",
    "sigma = config[\"telescope\"][\"psf\"][\"sigma\"]\n",
    "kernel = gaussian_kernel_2d(m, n, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "def apply_psf_impl(cube: jnp.ndarray, kernel) -> jnp.ndarray:\n",
    "\n",
    "    return jnp.transpose(jax.vmap(partial(convolve2d, mode = \"same\"), in_axes = (2, None))(\n",
    "        cube, \n",
    "        kernel,\n",
    "    ), (1, 2, 0))\n",
    "apply_psf = partial(apply_psf_impl, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## convolve lsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "sigma = config[\"telescope\"][\"lsf\"][\"sigma\"]\n",
    "telescope = get_telescope(config)\n",
    "wave_resolution = telescope.wave_res\n",
    "extend_factor = 12\n",
    "\n",
    "kernel = _get_kernel(sigma, wave_resolution, factor=extend_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "def apply_lsf_impl(cube: jnp.ndarray, kernel: jnp.array, extend_factor: int) -> jnp.ndarray:\n",
    "    reshaped_cube = cube.reshape(-1, cube.shape[-1])\n",
    "    convolved = jax.vmap(partial(convolve, mode=\"full\"), in_axes=(0, None))(reshaped_cube, kernel)\n",
    "    end = reshaped_cube.shape[1] + kernel.shape[0] - 1  - extend_factor\n",
    "    convolved= convolved[:, extend_factor:end]\n",
    "    return convolved.reshape(cube.shape)\n",
    "\n",
    "apply_lsf = partial(apply_lsf_impl, kernel=kernel, extend_factor=extend_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## apply noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "signal_to_noise = config[\"telescope\"][\"noise\"][\"signal_to_noise\"]\n",
    "\n",
    "# Get the noise distribution\n",
    "noise_distribution = config[\"telescope\"][\"noise\"][\"noise_distribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "def calculate_S2N(cube: jnp.ndarray, observation_s2n: float)->jnp.ndarray: \n",
    "    flux_image = jnp.sum(cube, axis=2)\n",
    "    return jnp.where(flux_image > 0 , (jnp.sqrt(jnp.median(jnp.where(flux_image > 0 , flux_image, 0.)))/observation_s2n)/jnp.sqrt(flux_image), 0)\n",
    "\n",
    "def apply_noise_impl(cube: jnp.array, signal_to_noise: float) -> jnp.ndarray:\n",
    "    # TODO: this can probably be vmapped for better performance\n",
    "    key = jrandom.PRNGKey(0)\n",
    "    s2n = calculate_S2N(cube, signal_to_noise)\n",
    "    return cube + cube*jrandom.normal(key, cube.shape) * s2n[:, :, None] \n",
    "\n",
    "apply_noise = partial(apply_noise_impl, signal_to_noise=signal_to_noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## build pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "looks like everything is in place now, so we can build pipelines for the data transformations and the cube transformations. This is only done for sake of debugging, in production the separation is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "@jax.jit\n",
    "def transform_data(inputdata: jnp.ndarray) -> jnp.ndarray:\n",
    "\n",
    "    data = rotate_galaxy(inputdata)\n",
    "    data = filter_particles(data)\n",
    "    data = spaxel_assignment(data)\n",
    "    data = calculate_spectra(data)\n",
    "    data = scale_spectrum_by_mass(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "this pipeline building and data prepare needs to go eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "pipe = RubixPipeline(config)\n",
    "inputdata = pipe.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = inputdata | Pipe(build_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "jax.debug.visualize_array_sharding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = transform_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data.block_until_ready();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "data.shape, data.nbytes// 1024**2, data.nbytes/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "jax.debug.visualize_array_sharding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "The data array is still correctly sharded. yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "when working with the cube pipeline now, we have to reshard it first and index into the padded cube or pad all the other data too. This is done in the `compute_cube` function using the first method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "def reshard_cube(cube: jnp.ndarray,) -> jnp.ndarray:\n",
    "    d = cube.shape[2]\n",
    "\n",
    "    # we can only go upwards to not loose\n",
    "    while d % jax.device_count() != 0:\n",
    "        d += 1\n",
    "    d\n",
    "    padding = d - cube.shape[2]\n",
    "    mesh = jax.make_mesh((jax.device_count(), ), ('devices',))\n",
    "    shard = NamedSharding(mesh, P(None, None, 'devices'))\n",
    "\n",
    "    cube = jax.device_put(jnp.pad(cube, ((0, 0), (0, 0), (0, padding))), shard)\n",
    "    return cube\n",
    "\n",
    "def compute_cube(inputdata: jnp.ndarray) -> jnp.ndarray:\n",
    "    cube = calculate_datacube(inputdata)\n",
    "    \n",
    "    # not sure if this counteracts the sharding\n",
    "    cube = apply_psf(cube)\n",
    "    cube = apply_lsf(cube)\n",
    "    cube = apply_noise(cube)\n",
    "    return cube\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "simple cube is not sharded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "cube = calculate_datacube(data)\n",
    "jax.debug.visualize_array_sharding(cube.reshape(cube.shape[0]* cube.shape[1], cube.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "cube = reshard_cube(cube)\n",
    "jax.debug.visualize_array_sharding(cube.reshape(cube.shape[0]* cube.shape[1], cube.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "I have not applied this to the computation now because it is messy to do and it's not the main objective. this data cube is tiny by comparison. What one has to do is pad the data that takes part in the computations in the cube pipeline to the size of the cube. then the sharding should be fine. indexing into the cube will destroy the sharding again apparently, distributing it over all devices in the case of this tiny one. not good... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "final_cube = compute_cube(data)\n",
    "final_cube.block_until_ready()\n",
    "jax.debug.visualize_array_sharding(final_cube.reshape(final_cube.shape[0]* final_cube.shape[1], final_cube.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "not sharded correctly... :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "\n",
    "final_cube.shape, final_cube.nbytes / 1024**2, final_cube.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "... but it's also really small, so might be that? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## memory usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "The main point: which function causes memory explosion and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "So far, we barely need 710 MB for the data cube, and we are not efficiently using memory at all. On multiple GPUs with overall O(100)GB, we should easily be able to process the required data sizes.\n",
    "\n",
    "**Expectation:**\n",
    "For the 500k particles, this would amount to roughly (500/30)*710 = 11833, so 12 GB. Even with with double the number of spectral lines we should easily be able to run this on a 4090. up to ~800k particles on a single GPU with the current spectral line number should also be doable, and we do not talk about sharding here at all. \n",
    "\n",
    "When we have gas, this goes down by half. At any rate, how can this computation cause memory issues on this gpu?\n",
    "\n",
    "**Observation**\n",
    "However, something temporarily causes a gigantic number of allocations in temporary arrays that lets memory usage go up to 40G or more. this is the killer element, I don't think that the sharding as such is a problem. \n",
    "\n",
    "Experiments above show that it's happening when processing the data itself, the cube computations are harmless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "check each function of the pipeline with htop/nvtop or similar tools: htop -d 3 --> update ever 0.3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = build_data(inputdata)\n",
    "data.block_until_ready(); # not the culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = rotate_galaxy(data)\n",
    "data.block_until_ready(); #not the culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = filter_particles(data)\n",
    "data.block_until_ready(); #not the culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = spaxel_assignment(data)\n",
    "data.block_until_ready(); #not the culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = calculate_spectra(data)\n",
    "data.block_until_ready(); # very much the culprit! increases memory size to > 40 GB even though the input is only ~0.7 - 0.8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = scale_spectrum_by_mass(data)\n",
    "data.block_until_ready(); #not the culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "data = resample(data)\n",
    "data.block_until_ready(); # moderate increase, not beyond a manageable size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "cube = calculate_datacube(data)\n",
    "cube.block_until_ready();  #not the culprit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "just to be sure: check cube computation agani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "final_cube = compute_cube(data)\n",
    "final_cube.block_until_ready();  #not the culprit at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "There is a big problem in the spectra calculation that causes an enormous temporary memory issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
