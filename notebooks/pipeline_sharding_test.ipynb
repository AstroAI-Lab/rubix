{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Logical cores (includes hyperthreads)\n",
    "print(\"Logical cores:\", os.cpu_count())\n",
    "\n",
    "\n",
    "# Total threads/cores via multiprocessing\n",
    "print(\"multiprocessing.cpu_count():\", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Tell XLA to fake 2 host CPU devices\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=3'\n",
    "\n",
    "# Only make GPU 0 and GPU 1 visible to JAX:\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '7, 8, 9'\n",
    "\n",
    "# for making sure that JAX doesnt'consume all memory at once\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]   = \"false\"\n",
    "\n",
    "import jax\n",
    "# Now JAX will list two CpuDevice entries\n",
    "print(jax.devices())\n",
    "# → [CpuDevice(id=0), CpuDevice(id=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "import os\n",
    "#  os.environ['SPS_HOME'] = '/mnt/storage/annalena_data/sps_fsps'\n",
    "#os.environ['SPS_HOME'] = '/home/annalena/sps_fsps'\n",
    "#os.environ['SPS_HOME'] = '/Users/annalena/Documents/GitHub/fsps'\n",
    "os.environ['SPS_HOME'] = '/home/hmack/.cache/fsps'\n",
    "os.environ['ILLUSTRIS_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# RUBIX pipeline\n",
    "\n",
    "RUBIX is designed as a linear pipeline, where the individual functions are called and constructed as a pipeline. This allows as to execute the whole data transformation from a cosmological hydrodynamical simulation of a galaxy to an IFU cube in two lines of code. This notebook shows, how to execute the pipeline on multiple machines. To see, how the pipeline is executed in small individual steps per individual function, we refer to the notebook `rubix_pipeline_stepwise.ipynb`.\n",
    "\n",
    "## How to use the Pipeline\n",
    "1) Define a `config`\n",
    "2) Setup the `pipeline yaml`\n",
    "3) Run the RUBIX pipeline\n",
    "4) Do science with the mock-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 1: Config\n",
    "\n",
    "The `config` contains all the information needed to run the pipeline. Those are run specfic configurations. Currently we just support Illustris as simulation, but extensions to other simulations (e.g. NIHAO) are planned.\n",
    "\n",
    "For the `config` you can choose the following options:\n",
    "- `pipeline`: you specify the name of the pipeline that is stored in the yaml file in rubix/config/pipeline_config.yml\n",
    "- `logger`: RUBIX has implemented a logger to report to the user, what is happening during the pipeline execution and give warnings\n",
    "- `data - args - particle_type`: load only stars particle (\"particle_type\": [\"stars\"]) or only gas particle (\"particle_type\": [\"gas\"]) or both (\"particle_type\": [\"stars\",\"gas\"])\n",
    "- `data - args - simulation`: choose the Illustris simulation (e.g. \"simulation\": \"TNG50-1\")\n",
    "- `data - args - snapshot`: which time step of the simulation (99 for present day)\n",
    "- `data - args - save_data_path`: set the path to save the downloaded Illustris data\n",
    "- `data - load_galaxy_args - id`: define, which Illustris galaxy is downloaded\n",
    "- `data - load_galaxy_args - reuse`: if True, if in the save_data_path directory a file for this galaxy id already exists, the downloading is skipped and the preexisting file is used\n",
    "- `data - subset`: only a defined number of stars/gas particles is used and stored for the pipeline. This may be helpful for quick testing\n",
    "- `simulation - name`: currently only IllustrisTNG is supported\n",
    "- `simulation - args - path`: where the data is stored and how the file will be named\n",
    "- `output_path`: where the hdf5 file is stored, which is then the input to the RUBIX pipeline\n",
    "- `telescope - name`: define the telescope instrument that is observing the simulation. Some telescopes are predefined, e.g. MUSE. If your instrument does not exist predefined, you can easily define your instrument in rubix/telescope/telescopes.yaml\n",
    "- `telescope - psf`: define the point spread function that is applied to the mock data\n",
    "- `telescope - lsf`: define the line spread function that is applied to the mock data\n",
    "- `telescope - noise`: define the noise that is applied to the mock data\n",
    "- `cosmology`: specify the cosmology you want to use, standard for RUBIX is \"PLANCK15\"\n",
    "- `galaxy - dist_z`: specify at which redshift the mock-galaxy is observed\n",
    "- `galaxy - rotation`: specify the orientation of the galaxy. You can set the types edge-on or face-on or specify the angles alpha, beta and gamma as rotations around x-, y- and z-axis\n",
    "- `ssp - template`: specify the simple stellar population lookup template to get the stellar spectrum for each stars particle. In RUBIX frequently \"BruzualCharlot2003\" is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "import matplotlib.pyplot as plt\n",
    "from rubix.core.pipeline import RubixPipeline \n",
    "import os\n",
    "\n",
    "config = {\n",
    "    \"pipeline\":{\"name\": \"calc_ifu\"},\n",
    "    \n",
    "    \"logger\": {\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"log_file_path\": None,\n",
    "        \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"name\": \"IllustrisAPI\",\n",
    "        \"args\": {\n",
    "            \"api_key\": os.environ.get(\"ILLUSTRIS_API_KEY\"),\n",
    "            \"particle_type\": [\"stars\"],\n",
    "            \"simulation\": \"TNG50-1\",\n",
    "            \"snapshot\": 99,\n",
    "            \"save_data_path\": \"data\",\n",
    "        },\n",
    "        \n",
    "        \"load_galaxy_args\": {\n",
    "        \"id\": 14,\n",
    "        \"reuse\": True,\n",
    "        },\n",
    "        \n",
    "        \"subset\": {\n",
    "            \"use_subset\": True,\n",
    "            \"subset_size\": 30000,\n",
    "        },\n",
    "    },\n",
    "    \"simulation\": {\n",
    "        \"name\": \"IllustrisTNG\",\n",
    "        \"args\": {\n",
    "            \"path\": \"data/galaxy-id-14.hdf5\",\n",
    "        },\n",
    "    \n",
    "    },\n",
    "    \"output_path\": \"output\",\n",
    "\n",
    "    \"telescope\":\n",
    "        {\"name\": \"MUSE\",\n",
    "         \"psf\": {\"name\": \"gaussian\", \"size\": 5, \"sigma\": 0.6},\n",
    "         \"lsf\": {\"sigma\": 0.5},\n",
    "         \"noise\": {\"signal_to_noise\": 100,\"noise_distribution\": \"normal\"},},\n",
    "    \"cosmology\":\n",
    "        {\"name\": \"PLANCK15\"},\n",
    "        \n",
    "    \"galaxy\":\n",
    "        {\"dist_z\": 0.1,\n",
    "         \"rotation\": {\"type\": \"edge-on\"},\n",
    "        },\n",
    "    \"ssp\": {\n",
    "        \"template\": {\n",
    "            \"name\": \"FSPS\"\n",
    "        },\n",
    "        \"dust\": {\n",
    "                \"extinction_model\": \"Cardelli89\",\n",
    "                \"dust_to_gas_ratio\": 0.01,\n",
    "                \"dust_to_metals_ratio\": 0.4,\n",
    "                \"dust_grain_density\": 3.5,\n",
    "                \"Rv\": 3.1,\n",
    "            },\n",
    "    },        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 2: Pipeline yaml\n",
    "\n",
    "To run the RUBIX pipeline, you need a yaml file (stored in `rubix/config/pipeline_config.yml`) that defines which functions are used during the execution of the pipeline. This shows the example pipeline yaml to compute a stellar IFU cube.\n",
    "\n",
    "```yaml\n",
    "calc_ifu:\n",
    "  Transformers:\n",
    "    rotate_galaxy:\n",
    "      name: rotate_galaxy\n",
    "      depends_on: null\n",
    "      args: []\n",
    "      kwargs:\n",
    "        type: \"face-on\"\n",
    "    filter_particles:\n",
    "      name: filter_particles\n",
    "      depends_on: rotate_galaxy\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    spaxel_assignment:\n",
    "      name: spaxel_assignment\n",
    "      depends_on: filter_particles\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    reshape_data:\n",
    "      name: reshape_data\n",
    "      depends_on: spaxel_assignment\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    calculate_spectra:\n",
    "      name: calculate_spectra\n",
    "      depends_on: reshape_data\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    scale_spectrum_by_mass:\n",
    "      name: scale_spectrum_by_mass\n",
    "      depends_on: calculate_spectra\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    doppler_shift_and_resampling:\n",
    "      name: doppler_shift_and_resampling\n",
    "      depends_on: scale_spectrum_by_mass\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    calculate_datacube:\n",
    "      name: calculate_datacube\n",
    "      depends_on: doppler_shift_and_resampling\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    convolve_psf:\n",
    "      name: convolve_psf\n",
    "      depends_on: calculate_datacube\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    convolve_lsf:\n",
    "      name: convolve_lsf\n",
    "      depends_on: convolve_psf\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "    apply_noise:\n",
    "      name: apply_noise\n",
    "      depends_on: convolve_lsf\n",
    "      args: []\n",
    "      kwargs: {}\n",
    "```\n",
    "\n",
    "There is one thing you have to know about the naming of the functions in this yaml: To use the functions inside the pipeline, the functions have to be called exactly the same as they are returned from the core module function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "pipe = RubixPipeline(config)\n",
    "inputdata = pipe.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec as P, NamedSharding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "mesh = jax.make_mesh((jax.device_count(), ), ('x',))\n",
    "shard = NamedSharding(mesh, P('x'))\n",
    "data = jax.device_put(inputdata, shard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "why this no work?? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "try simpler approach for this thing for now. This is really stupid: just build a giant box of zeros, index into them in the right way, and use these indices to assign the values we want to slices in the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function builds the data from the rubixdata object because that is easiest, but should not really be done imho. \n",
    "def build_data(input): \n",
    "    long_axis = input.stars.age.shape[0]\n",
    "    data = jnp.zeros((long_axis, 6200), dtype=jnp.float32)\n",
    "    inputdata.galaxy.redshift = jnp.float32(inputdata.galaxy.redshift)\n",
    "    inputdata.galaxy.halfmassrad_stars = jnp.array(inputdata.galaxy.halfmassrad_stars, dtype=jnp.float32)\n",
    "    inputdata.galaxy.center = jnp.array(inputdata.galaxy.center, dtype=jnp.float32)\n",
    "\n",
    "    inputdata.stars.coords = jnp.array(inputdata.stars.coords, dtype=jnp.float32)\n",
    "    inputdata.stars.age = jnp.array(inputdata.stars.age, dtype=jnp.float32)\n",
    "    inputdata.stars.velocity = jnp.array(inputdata.stars.velocity, dtype=jnp.float32)\n",
    "    inputdata.stars.metallicity = jnp.array(inputdata.stars.metallicity, dtype=jnp.float32)\n",
    "    inputdata.stars.mass = jnp.array(inputdata.stars.mass, dtype=jnp.float32)\n",
    "    # stars properties\n",
    "    data = data.at[:, 0:3].set(inputdata.stars.coords)\n",
    "    data = data.at[:, 3:6].set(inputdata.stars.velocity)\n",
    "    data = data.at[:, 6].set(inputdata.stars.metallicity)\n",
    "    data = data.at[:, 7].set(inputdata.stars.age)\n",
    "    data = data.at[:, 8].set(inputdata.stars.mass)\n",
    "\n",
    "    # galaxy properties\n",
    "    data = data.at[:, 9].set(inputdata.galaxy.halfmassrad_stars)\n",
    "    data = data.at[:, 10].set(inputdata.galaxy.redshift)\n",
    "    data = data.at[:, 11:14].set(inputdata.galaxy.center)\n",
    "    \n",
    "    mesh = jax.make_mesh((jax.device_count(), ), ('x',))\n",
    "    shard = NamedSharding(mesh, P('x'))\n",
    "\n",
    "    data = jax.device_put(data, shard)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stars(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Stars function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    # Perform some operations on the data\n",
    "    # For example, let's just return the data as is\n",
    "    return data[:, 0:9]\n",
    "\n",
    "def galaxy(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Galaxy function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    # Perform some operations on the data\n",
    "    # For example, let's just return the data as is\n",
    "    return data[:, 9:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def coords_idx(): \n",
    "    return jnp.s_[:, 0:3]\n",
    "\n",
    "def coords(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Coords function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[coords_idx()]\n",
    "\n",
    "def velocity_idx():\n",
    "    return jnp.s_[:, 3:6]\n",
    "\n",
    "def velocity(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Velocity function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[velocity_idx()]\n",
    "\n",
    "def metallicity_idx():\n",
    "    return jnp.s_[:, 6]\n",
    "\n",
    "def metallicity(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Metallicity function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[metallicity_idx()]\n",
    "\n",
    "def age_idx():\n",
    "    return jnp.s_[:, 7]\n",
    "\n",
    "def age(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Age function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[age_idx()]\n",
    "\n",
    "def mass_idx():\n",
    "    return jnp.s_[:, 8]\n",
    "\n",
    "def mass(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Age function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[mass_idx()]\n",
    "\n",
    "def halfmassrad_stars_idx():\n",
    "    return jnp.s_[:, 9]\n",
    "\n",
    "def halfmassrad_stars(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Halfmassrad_stars function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[halfmassrad_stars_idx()]\n",
    "\n",
    "\n",
    "def redshift_idx():\n",
    "    return jnp.s_[:, 10]\n",
    "\n",
    "def redshift(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Redshift function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[redshift_idx()]\n",
    "\n",
    "def center_idx():\n",
    "    return jnp.s_[:, 11:14]\n",
    "\n",
    "def center(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Center function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[center_idx()]\n",
    "\n",
    "def mask_idx() :\n",
    "    return jnp.s_[:, 14]\n",
    "\n",
    "def mask(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Mask function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[mask_idx()]\n",
    "\n",
    "def pixel_assignment_idx() : \n",
    "    return jnp.s_[:, 15]\n",
    "\n",
    "def pixel_assignment(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Pixel assignment function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[pixel_assignment_idx()]\n",
    "\n",
    "\n",
    "def spectra_index(): \n",
    "    return jnp.s_[:, 16:(16 + 5994)]\n",
    "\n",
    "def spectra(data: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Spectra function to be used in the pipeline.\n",
    "    \"\"\"\n",
    "    return data[spectra_index()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_data(inputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "try the sharding now with pipeline functions. since the pipeline functions use other data, I don´t use them directly, but build simplified versions here that only include stars. this involves the build up of the pipeline from the ground up in such a way that the data is sharded once and then we don´t have to touch it again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "TODO: make sure the functions have the correct static argnums such that we don´t have to worry about the tracing shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pipe import Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "galaxy rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubix.galaxy.alignment import moment_of_inertia_tensor, rotation_matrix_from_inertia_tensor, apply_init_rotation, apply_rotation\n",
    "\n",
    "def rotate_galaxy_impl(data: jnp.array, alpha, beta, gamma)->jnp.array: \n",
    "\n",
    "    I = moment_of_inertia_tensor(coords(data), mass(data), halfmassrad_stars(data),)\n",
    "    R = rotation_matrix_from_inertia_tensor(I)\n",
    "    data = data.at[coords_idx()].set(apply_rotation(apply_init_rotation(coords(data), R), alpha, beta, gamma))\n",
    "    data = data.at[velocity_idx()].set(apply_rotation(apply_init_rotation(velocity(data), R), alpha, beta, gamma))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rotate_galaxy_impl(data, 0.1, 0.2, 0.3)\n",
    "type(r), r.shape, r.dtype, r.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_galaxy = partial(rotate_galaxy_impl, alpha=90.0, beta=0.0, gamma=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "filter particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubix.core.telescope import get_spatial_bin_edges\n",
    "from rubix.telescope.utils import mask_particles_outside_aperture\n",
    "\n",
    "\n",
    "def filter_particles_impl(data: jnp.ndarray, spatial_bin_edges) -> jnp.ndarray:\n",
    "    mask = mask_particles_outside_aperture(\n",
    "        coords(data), spatial_bin_edges\n",
    "    )\n",
    "\n",
    "    data = data.at[mask_idx()].set(mask)\n",
    "\n",
    "    for attr in [age_idx, mass_idx, metallicity_idx, ]: \n",
    "        data = data.at[attr()].set(\n",
    "            jnp.where(mask, data[attr()], 0)\n",
    "        )\n",
    "\n",
    "    return data\n",
    "\n",
    "filter_particles = partial(filter_particles_impl, spatial_bin_edges=get_spatial_bin_edges(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_spatial_bin_edges(config).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter_particles(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "try out simple pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputdata | Pipe(build_data) | Pipe(rotate_galaxy) | Pipe(filter_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, data.nbytes / 1024**2, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "try to compile it and run it then,then check sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubix.core.pipeline import RubixPipeline \n",
    "from rubix.core.data import RubixData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit \n",
    "def pipeline(data: jnp.array) -> jnp.ndarray:\n",
    "    data = rotate_galaxy(data)\n",
    "    data = filter_particles(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_data(inputdata)\n",
    "data = pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, data.nbytes / 1024**2, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "spaxel assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaxel_assignment_square_impl(data: jnp.ndarray, spatial_bin_edges)-> jnp.ndarray:\n",
    "    # Calculate assignment of of x and y coordinates to bins separately\n",
    "    x_indices = (\n",
    "        jnp.digitize(data[coords_idx()][:, 0], spatial_bin_edges) - 1\n",
    "    )  # -1 to start indexing at 0\n",
    "    y_indices = jnp.digitize(data[coords_idx()][:, 1], spatial_bin_edges) - 1\n",
    "\n",
    "    number_of_bins = len(spatial_bin_edges) - 1\n",
    "\n",
    "    # Clip the indices to the valid range\n",
    "    x_indices = jnp.clip(x_indices, 0, number_of_bins - 1)\n",
    "    y_indices = jnp.clip(y_indices, 0, number_of_bins - 1)\n",
    "\n",
    "    # Flatten the 2D indices to 1D indices\n",
    "    pixel_positions = x_indices + (number_of_bins * y_indices)\n",
    "    return data.at[pixel_assignment_idx()].set(jnp.round(pixel_positions))\n",
    "\n",
    "\n",
    "spaxel_assignment = partial(spaxel_assignment_square_impl, spatial_bin_edges=get_spatial_bin_edges(config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "try it out again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputdata | Pipe(build_data) | Pipe(rotate_galaxy) | Pipe(filter_particles) | Pipe(spaxel_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "calculate spectra now. since this is so big, it would perpaps make sense to have a separate path for this thing instead of having to save this and drag it around all the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubix.core.ssp import get_ssp, get_lookup_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spectra_impl(data: jnp.ndarray, lookup_interpolation) -> jnp.ndarray: \n",
    "\n",
    "    # this thing is gigantic and probably cannot be stored in memory for serious data\n",
    "    return data.at[spectra_index()].set(lookup_interpolation(\n",
    "        data[metallicity_idx()],\n",
    "        data[age_idx()],\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_interpolation = get_lookup_interpolation(config)\n",
    "\n",
    "calculate_spectra = partial(calculate_spectra_impl, lookup_interpolation=lookup_interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputdata | Pipe(build_data) | Pipe(rotate_galaxy) | Pipe(filter_particles) | Pipe(spaxel_assignment) | Pipe(calculate_spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data), data.shape, data.dtype, data.nbytes / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "scale spectrum by mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_spectrum_by_mass(data: jnp.ndarray) -> jnp.ndarray:\n",
    "\n",
    "    return data.at[spectra_index()].set(\n",
    "        data[spectra_index()] * data[mass_idx()][:, jnp.newaxis]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scale_spectrum_by_mass(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data), data.shape, data.dtype, data.nbytes / 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "So far, we barely need 710 MB for everything we do, and we are not efficient at all wrt memory. On multiple GPUs with overall 100GB, we should easily be able to process the required data sizes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "doppler shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the needed crap... \n",
    "from rubix import config as rubix_config\n",
    "velocity_direction = rubix_config[\"ifu\"][\"doppler\"][\"velocity_direction\"]\n",
    "directions = {\"x\": 0, \"y\": 1, \"z\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_doppler_impl(data: jnp.ndarray, wavelength, c, direction) -> jnp.ndarray:\n",
    "    print(\"shapes: \", data[velocity_idx()].shape, wavelength.shape)\n",
    "\n",
    "    # FIXME: this needs to be vmapped or broadcasted in such a way that every velocity component is doppler shifted for each wavelength. \n",
    "    # calculate classic doppler shift \n",
    "    v = data[velocity_idx()][:, direction]\n",
    "    return data.at[velocity_idx()][:, direction].set(\n",
    "        wavelength * jnp.exp(v/c)\n",
    "    )\n",
    "\n",
    "ssp = get_ssp(config)\n",
    "ssp_wave= ssp.wavelength\n",
    "direction = directions[velocity_direction]\n",
    "cosmological_doppler_shift = (1 + config[\"galaxy\"][\"dist_z\"]) * ssp.wavelength\n",
    "\n",
    "apply_doppler = partial(apply_doppler_impl, wavelength=ssp_wave, c=3e8, direction=direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_doppler(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
