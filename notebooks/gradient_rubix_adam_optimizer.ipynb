{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient calculation\n",
    "\n",
    "In this notebook we show, how you can calculate teh full Jacobian of the pipeline with respect to the input data.\n",
    "\n",
    "First of all, we define our config as input for the pipeline, set up the pipeline and let it run as we du normally the forward modeling, so downloading IllustrisTNG data and transforming the data in the linear pipeline and end up with the datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubix.core.pipeline import RubixPipeline\n",
    "# Suppose you already have a user_config or path to config\n",
    "#config = \"../rubix/config/pipeline_config.yaml\"\n",
    "import os\n",
    "config = {\n",
    "    \"pipeline\":{\"name\": \"calc_gradient\"},\n",
    "    \n",
    "    \"logger\": {\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"log_file_path\": None,\n",
    "        \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"name\": \"IllustrisAPI\",\n",
    "        \"args\": {\n",
    "            \"api_key\": os.environ.get(\"ILLUSTRIS_API_KEY\"),\n",
    "            \"particle_type\": [\"stars\"],\n",
    "            \"simulation\": \"TNG50-1\",\n",
    "            \"snapshot\": 99,\n",
    "            \"save_data_path\": \"data\",\n",
    "        },\n",
    "        \n",
    "        \"load_galaxy_args\": {\n",
    "        \"id\": 14,\n",
    "        \"reuse\": True,\n",
    "        },\n",
    "        \n",
    "        \"subset\": {\n",
    "            \"use_subset\": True,\n",
    "            \"subset_size\": 2,\n",
    "        },\n",
    "    },\n",
    "    \"simulation\": {\n",
    "        \"name\": \"IllustrisTNG\",\n",
    "        \"args\": {\n",
    "            \"path\": \"data/galaxy-id-14.hdf5\",\n",
    "        },\n",
    "    \n",
    "    },\n",
    "    \"output_path\": \"output\",\n",
    "    \"output_modified\":  False,\n",
    "\n",
    "    \"telescope\":\n",
    "        {\"name\": \"TESTGRADIENT\",\n",
    "         \"psf\": {\"name\": \"gaussian\", \"size\": 5, \"sigma\": 0.6},\n",
    "         \"lsf\": {\"sigma\": 0.5},\n",
    "         \"noise\": {\"signal_to_noise\": 1,\"noise_distribution\": \"normal\"},\n",
    "         },\n",
    "    \"cosmology\":\n",
    "        {\"name\": \"PLANCK15\"},\n",
    "        \n",
    "    \"galaxy\":\n",
    "        {\"dist_z\": 0.1,\n",
    "         \"rotation\": {\"type\": \"edge-on\"},\n",
    "        },\n",
    "        \n",
    "    \"ssp\": {\n",
    "        \"template\": {\n",
    "            \"name\": \"BruzualCharlot2003\"\n",
    "        },\n",
    "    },        \n",
    "}\n",
    "pipe = RubixPipeline(config)\n",
    "rubixdata = pipe.run()\n",
    "\n",
    "#target_datacube = rubixdata.stars.datacube\n",
    "#target_age = rubixdata.stars.age\n",
    "#target_metallicity = rubixdata.stars.metallicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rubixdata as input for the pipeline\n",
    "\n",
    "For gradient based optimization, it would be good to give the pipeline directly the modified rubixdata object and calculate the new datacube. This is now possible, if you load the pipeline from rubix.core.pipeline_gradient instead of rubix.core.pipeline. You set up the Pipeline and then you pass the rubixdata to the run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "from rubix.core.pipeline_gradient import RubixPipeline\n",
    "# Suppose you already have a user_config or path to config\n",
    "#config = \"../rubix/config/pipeline_config.yaml\"\n",
    "import os\n",
    "config = {\n",
    "    \"pipeline\":{\"name\": \"calc_gradient\"},\n",
    "    \n",
    "    \"logger\": {\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"log_file_path\": None,\n",
    "        \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    },\n",
    "     \"data\": {\n",
    "        \"args\": {\n",
    "            \"particle_type\": [\"stars\"],\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    \"output_path\": \"output\",\n",
    "    \"output_modified\":  False,\n",
    "\n",
    "    \"telescope\":\n",
    "        {\"name\": \"TESTGRADIENT\",\n",
    "         \"psf\": {\"name\": \"gaussian\", \"size\": 5, \"sigma\": 0.6},\n",
    "         \"lsf\": {\"sigma\": 0.5},\n",
    "         \"noise\": {\"signal_to_noise\": 1,\"noise_distribution\": \"normal\"},\n",
    "         },\n",
    "    \"cosmology\":\n",
    "        {\"name\": \"PLANCK15\"},\n",
    "        \n",
    "    \"galaxy\":\n",
    "        {\"dist_z\": 0.1,\n",
    "         \"rotation\": {\"type\": \"edge-on\"},\n",
    "        },\n",
    "        \n",
    "    \"ssp\": {\n",
    "        \"template\": {\n",
    "            \"name\": \"BruzualCharlot2003\"\n",
    "        },\n",
    "    },        \n",
    "}\n",
    "pipe = RubixPipeline(config)\n",
    "rubixdata = pipe.run(rubixdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wave = pipe.telescope.wave_seq\n",
    "\n",
    "spectra = rubixdata.stars.datacube\n",
    "#target_spectra = target_datacube[0,0,:]\n",
    "plt.plot(wave, spectra[0,0,:], label=\"current spectrum\")\n",
    "#plt.plot(wave, target_spectra, label=\"target\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient calculation\n",
    "\n",
    "The RubixPipeline from rubix.core.pipeline_gradient has als the gradient function implemented. You can just call the function and pass your rubixdata and then you get the gradfient returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "gradient = pipe.gradient(rubixdata)\n",
    "\n",
    "gradient_age = gradient.stars.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "rubixdata.stars.age = jnp.array([5.0, 5.0])\n",
    "rubixdata.stars.metallicity = jnp.array([0.005, 0.005])\n",
    "\n",
    "#calculate datacube with new age and metallicity\n",
    "target_rubixdata = pipe.run(rubixdata)\n",
    "target_datacube = target_rubixdata.stars.datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('output/loss/data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "age_vals = data['age_vals']\n",
    "metallicity_vals = data['metallicity_vals']\n",
    "losses = data['losses']\n",
    "grad_ages = data['grad_ages']\n",
    "grad_metallicities = data['grad_metallicities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# Define the optimization function\n",
    "def rubix_loss(params, rubixdata):\n",
    "    age, metallicity = params\n",
    "    rubixdata.stars.age = jnp.array([5.0, age])\n",
    "    rubixdata.stars.metallicity = jnp.array([0.005, metallicity])\n",
    "    rubixdata = pipe.run(rubixdata)\n",
    "    return pipe.loss_mse(rubixdata.stars.datacube, target_datacube)\n",
    "\n",
    "def gradient_descent_optimization(func, x_init, learning_rate=0.001, tol=1e-3, max_iter=100):\n",
    "    xlist = []\n",
    "    loss_list = []\n",
    "    x = x_init\n",
    "    xlist.append(x)\n",
    "\n",
    "    optimizer = optax.adam(learning_rate=learning_rate)\n",
    "    optimizer_state = optimizer.init(x)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        loss, grad = jax.value_and_grad(func)(x, rubixdata)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        updates, optimizer_state = optimizer.update(grad, optimizer_state)\n",
    "        x = optax.apply_updates(x, updates)\n",
    "        \n",
    "        # Clip to enforce constraints\n",
    "        x = jnp.array([\n",
    "            jnp.clip(x[0], 0.0, 10.3),      # Age constraint\n",
    "            jnp.clip(x[1], 1e-4, 0.05),    # Metallicity constraint\n",
    "        ])\n",
    "\n",
    "        xlist.append(x)\n",
    "\n",
    "        if jnp.linalg.norm(updates) < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    return x, jnp.array(xlist), jnp.array(loss_list)\n",
    "\n",
    "# Initial parameters\n",
    "x_init1 = jnp.array([10.0, 0.05])  # Initial age and metallicity\n",
    "x_init2 = jnp.array([0.0, 0.05])  # Initial age and metallicity\n",
    "x_init3 = jnp.array([6.0, 0.006])  # Initial age and metallicity\n",
    "x_init4 = jnp.array([0.0, 0.0001])  # Initial age and metallicity\n",
    "x_init5 = jnp.array([10.0, 0.0001])  # Initial age and metallicity\n",
    "learning_rate = 0.1\n",
    "tolerance = 1e-3\n",
    "max_iterations = 500\n",
    "\n",
    "# Run gradient descent\n",
    "optimized_params1, param_history1, loss_history1 = gradient_descent_optimization(\n",
    "    rubix_loss, x_init1, learning_rate=learning_rate, tol=tolerance, max_iter=max_iterations\n",
    ")\n",
    "optimized_params2, param_history2, loss_history2 = gradient_descent_optimization(\n",
    "    rubix_loss, x_init2, learning_rate=learning_rate, tol=tolerance, max_iter=max_iterations\n",
    ")\n",
    "optimized_params3, param_history3, loss_history3 = gradient_descent_optimization(\n",
    "    rubix_loss, x_init3, learning_rate=learning_rate, tol=tolerance, max_iter=max_iterations\n",
    ")\n",
    "optimized_params4, param_history4, loss_history4 = gradient_descent_optimization(\n",
    "    rubix_loss, x_init4, learning_rate=learning_rate, tol=tolerance, max_iter=max_iterations\n",
    ")\n",
    "optimized_params5, param_history5, loss_history5 = gradient_descent_optimization(\n",
    "    rubix_loss, x_init5, learning_rate=learning_rate, tol=tolerance, max_iter=max_iterations\n",
    ")\n",
    "\n",
    "# Extract the results\n",
    "optimized_age, optimized_metallicity = optimized_params1\n",
    "print(f\"Optimized Age: {optimized_age}\")\n",
    "print(f\"Optimized Metallicity: {optimized_metallicity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history1, label=\"Loss 1\")\n",
    "plt.plot(loss_history2, label=\"Loss 2\")\n",
    "plt.plot(loss_history3, label=\"Loss 3\")\n",
    "plt.plot(loss_history4, label=\"Loss 4\")\n",
    "plt.plot(loss_history5, label=\"Loss 5\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the parameter ranges\n",
    "age_values = jnp.linspace(0, 10.30103, 20)\n",
    "metallicity_values = jnp.logspace(np.log10(1e-4), np.log10(0.05), 20)\n",
    "\n",
    "# Reshape data for plotting\n",
    "age_grid, metallicity_grid = np.meshgrid(\n",
    "    np.unique(age_vals), np.unique(metallicity_vals)\n",
    ")\n",
    "loss_grid = losses.reshape(metallicity_grid.shape)\n",
    "grad_age_grid = grad_ages.reshape(metallicity_grid.shape)\n",
    "grad_metallicity_grid = grad_metallicities.reshape(metallicity_grid.shape)\n",
    "\n",
    "\n",
    "# Plotting the trajectory in the parameter space\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(\n",
    "    loss_grid,\n",
    "    extent=[age_values[0], age_values[-1], metallicity_values[0], metallicity_values[-1]],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"jet\",\n",
    ")\n",
    "plt.colorbar(label=\"Loss\")\n",
    "\n",
    "# Set log scale for the y-axis\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Add a red dot at age = 5 and metallicity = 0.005\n",
    "plt.scatter(5, 0.005, color=\"red\", label=\"Point (5, 0.005)\", zorder=200)\n",
    "\n",
    "plt.plot(param_history1[:, 0], param_history1[:, 1], 'r', label=\"Optimization Path 1\")\n",
    "plt.plot(param_history2[:, 0], param_history2[:, 1], 'g', label=\"Optimization Path 2\")\n",
    "plt.plot(param_history3[:, 0], param_history3[:, 1], 'b', label=\"Optimization Path 3\")\n",
    "plt.plot(param_history4[:, 0], param_history4[:, 1], 'y', label=\"Optimization Path 4\")\n",
    "plt.plot(param_history5[:, 0], param_history5[:, 1], 'm', label=\"Optimization Path 5\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Metallicity (log scale)\")\n",
    "plt.title(\"Loss Heatmap with Optimization Path\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_history1[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenberg-Marquart from the optimistix package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimistix import ScipyLeastSquares\n",
    "\n",
    "loss_history = []\n",
    "param_history = []\n",
    "\n",
    "def residual_fn(params):\n",
    "    param_history.append(params)  # Track the parameters\n",
    "    age, metallicity = params\n",
    "    rubixdata.stars.age = jnp.array([5.0, age])\n",
    "    rubixdata.stars.metallicity = jnp.array([0.005, metallicity])\n",
    "    rubixdata = pipe.run(rubixdata)\n",
    "    residuals = rubixdata.stars.datacube - target_datacube\n",
    "    loss_history.append(jnp.sum(residuals**2))  # Track the loss\n",
    "    return residuals.flatten()  # Residuals as a vector\n",
    "\n",
    "# Initial guess\n",
    "initial_guess = jnp.array([5.0, 0.005])\n",
    "\n",
    "# Run the optimizer\n",
    "optimizer = ScipyLeastSquares(residual_fn)\n",
    "optimized_params = optimizer.run(initial_guess)\n",
    "\n",
    "print(\"Optimized Age:\", optimized_params[0])\n",
    "print(\"Optimized Metallicity:\", optimized_params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert parameter history to numpy array\n",
    "param_history = jnp.array(param_history)\n",
    "\n",
    "# Plot the trajectory\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(param_history[:, 0], param_history[:, 1], 'r.-', label=\"Trajectory\")\n",
    "plt.scatter(optimized_params[0], optimized_params[1], color='blue', label=\"Optimal Point\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Metallicity\")\n",
    "plt.title(\"Optimization Trajectory in Parameter Space\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a loss grid\n",
    "age_values = jnp.linspace(0, 10.3, 50)\n",
    "metallicity_values = jnp.logspace(np.log10(1e-4), np.log10(0.05), 50)\n",
    "loss_grid = jnp.zeros((len(age_values), len(metallicity_values)))\n",
    "\n",
    "for i, age in enumerate(age_values):\n",
    "    for j, metallicity in enumerate(metallicity_values):\n",
    "        rubixdata.stars.age = jnp.array([5.0, age])\n",
    "        rubixdata.stars.metallicity = jnp.array([0.005, metallicity])\n",
    "        rubixdata = pipe.run(rubixdata)\n",
    "        residuals = rubixdata.stars.datacube - target_datacube\n",
    "        loss_grid[i, j] = jnp.sum(residuals**2)\n",
    "\n",
    "# Plot the heatmap with trajectory\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(\n",
    "    loss_grid.T,\n",
    "    extent=[age_values[0], age_values[-1], metallicity_values[0], metallicity_values[-1]],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "plt.colorbar(label=\"Loss\")\n",
    "plt.plot(param_history[:, 0], param_history[:, 1], 'r.-', label=\"Optimization Path\")\n",
    "plt.scatter(optimized_params[0], optimized_params[1], color='blue', label=\"Optimal Point\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Metallicity\")\n",
    "plt.title(\"Loss Landscape and Optimization Path\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
